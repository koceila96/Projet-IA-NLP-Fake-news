{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vtEN47uuCyKj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "import numpy as np\n",
        "#from gensim.models import Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_COL4rymCyKm"
      },
      "outputs": [],
      "source": [
        "import_folder = '/Users/soufianebelhabibe/Desktop/IA-project/data/data_prep/'\n",
        "\n",
        "X_train = pd.read_csv(import_folder + 'X_train.csv')\n",
        "X_test = pd.read_csv(import_folder + 'X_test.csv')\n",
        "y_train = pd.read_csv(import_folder + 'y_train.csv')\n",
        "y_test = pd.read_csv(import_folder + 'y_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape :(35416, 1)\n",
            "X_test  shape :(8847, 1)\n",
            "y_train shape :(35918, 1)\n",
            "y_test shape : (8980, 1)\n"
          ]
        }
      ],
      "source": [
        "print(f\"X_train shape :{X_train.shape}\")\n",
        "print(f\"X_test  shape :{X_test.shape}\")\n",
        "print(f\"y_train shape :{y_train.shape}\")\n",
        "print(f\"y_test shape : {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYj7DREuCyKq"
      },
      "source": [
        "# `TF-idf`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beWeN4nMCyKr",
        "outputId": "349a06e0-6a5c-4838-d3a3-c6d894754421"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vfv2ZkFhCyKs",
        "outputId": "6f43250c-4f5c-4992-92db-f2c83f70da6c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>washington reuters  us presidentelect donald t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what the what  i had to start firing people pe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>washington reuters  the white house plans to a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>first lady melania trump was trapped inside he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>united nations reuters  the united nations sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35411</th>\n",
              "      <td>washington reuters  president donald trump on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35412</th>\n",
              "      <td>infamous italian dictator benito mussolini ant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35413</th>\n",
              "      <td>a caller to british radio host katie hopkins w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35414</th>\n",
              "      <td>since citizens united and even before two name...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35415</th>\n",
              "      <td>when it comes to political campaign strategies...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35358 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text\n",
              "0      washington reuters  us presidentelect donald t...\n",
              "1      what the what  i had to start firing people pe...\n",
              "2      washington reuters  the white house plans to a...\n",
              "3      first lady melania trump was trapped inside he...\n",
              "4      united nations reuters  the united nations sec...\n",
              "...                                                  ...\n",
              "35411  washington reuters  president donald trump on ...\n",
              "35412  infamous italian dictator benito mussolini ant...\n",
              "35413  a caller to british radio host katie hopkins w...\n",
              "35414  since citizens united and even before two name...\n",
              "35415  when it comes to political campaign strategies...\n",
              "\n",
              "[35358 rows x 1 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77AeQ0pwCyKs"
      },
      "outputs": [],
      "source": [
        "'''xv_train= text_vectorization(X_train, \"text\")\n",
        "xv_test = text_vectorization(X_test, \"text\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EcD1fiZ5CyKt"
      },
      "outputs": [],
      "source": [
        "vectorization = TfidfVectorizer()\n",
        "xv_train1 = vectorization.fit_transform(X_train[\"text\"])\n",
        "xv_test1 = vectorization.fit_transform(X_test[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dwD3t3v2CyKt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['aa', 'aaa', 'aaaahhhh', ..., 'zyuganov', 'zz', 'zzzzaaaacccchhh'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorization.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "LR = LogisticRegression()\n",
        "LR.fit(xv_train1, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq3jZYCHCyKt"
      },
      "outputs": [],
      "source": [
        "'''data_tfidf=pd.DataFrame(xv_train1, columns=vectorization.get_feature_names_out())\n",
        "data_tfidf '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J_oQZnOCyKu"
      },
      "source": [
        "## Export the data if TFidf result (context) is the best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkMh_GS3CyKu"
      },
      "outputs": [],
      "source": [
        "from operator import index\n",
        "\n",
        "\n",
        "export_folder = \"/Users/soufianebelhabibe/Desktop/IA-project/data/data_prep/data_embed/\"\n",
        "xv_train1.to_csv(export_folder+'xv_train.csv', index=False)\n",
        "xv_test1.to_csv(export_folder+'xv_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1FJPNiKCyKv"
      },
      "source": [
        "# `CBOW - WORD2VEC - GENSIM`\n",
        "* [Documentation](https://radimrehurek.com/gensim/models/word2vec.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhKA9rWERXhI",
        "outputId": "c8d97260-df75-4605-e4f0-5130abb97739"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Téléchargez les mots vides si vous ne les avez pas déjà téléchargés\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Définir la liste des mots vides\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e7EN7lriRFHf"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    tokens = text.split()  # Tokenization\n",
        "    tokens = [token for token in tokens if token not in stop_words]  # Suppression des mots vides\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM6-zPrcYUL2",
        "outputId": "7c09fc8e-1e6b-4162-fd6a-287a767a1ac9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                  0         1         2         3         4         5  \\\n",
              " said       0.700145 -0.259737  0.203915  2.887067  2.577803 -0.613257   \n",
              " trump      0.928114 -4.784083 -3.736300 -0.190721 -1.321421  3.048606   \n",
              " us        -0.769878 -0.012436  0.166623 -1.909896 -0.002877  0.924405   \n",
              " would      1.728843 -1.130222  1.523448  1.311536 -2.190951  2.282484   \n",
              " president -0.494144 -2.690189 -4.008368 -1.083082 -1.576218  3.817541   \n",
              " \n",
              "                   6         7         8         9  \n",
              " said       1.694138  1.787241  0.958228  3.409619  \n",
              " trump      1.378335  0.746198 -1.803092  0.970298  \n",
              " us         1.558431  1.876669 -0.492233  3.385955  \n",
              " would     -0.111693  1.316125  0.517965  3.850860  \n",
              " president  3.895561  3.602871  0.444994  2.067512  ,\n",
              " (32999, 10))"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = X_test[\"text\"]\n",
        "tokenized_tweet0 = data.apply(preprocess_text)\n",
        "\n",
        "size=10\n",
        "model_w2v = Word2Vec(\n",
        "            tokenized_tweet0, #l'ensemble des tweet\n",
        "            window=5, # context window size\n",
        "            vector_size=size,  #La taille de sortie\n",
        "            min_count=2, # Ignores all words with total frequency lower than 2.\n",
        "            sg = 0) #on utilise la methode CBOW\n",
        "\n",
        "model_w2v.train(tokenized_tweet0, total_examples= len(data), epochs=20)\n",
        "\n",
        "#etape5 je stocke mes resultats en dataframe\n",
        "words=list(model_w2v.wv.key_to_index)\n",
        "data_emb1=pd.DataFrame(model_w2v.wv[list(model_w2v.wv.key_to_index)],index=words)\n",
        "\n",
        "data_emb1.head() , data_emb1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb0OzneKCyKv",
        "outputId": "20b13863-c98c-4e92-f9e4-29c6e9b8c266"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                  0         1         2         3         4         5  \\\n",
              " said      -3.074607  0.272230 -2.285322  0.181961  0.229743  2.807336   \n",
              " trump      0.945750  0.338583 -0.019658 -6.579536 -0.049426  0.685794   \n",
              " us        -1.272605  0.367515  0.850686 -0.830141 -1.625525  2.546678   \n",
              " would     -0.658398 -1.562374 -2.557191 -1.578615 -4.096815  0.912180   \n",
              " president -1.352463  0.205018  0.035683 -5.800277  0.909239  1.807892   \n",
              " \n",
              "                   6         7         8         9  \n",
              " said       0.343344  2.084930 -2.581864  0.174442  \n",
              " trump      0.846598  2.210617 -1.345544  0.589007  \n",
              " us         1.672901 -0.046529 -2.228971  1.413272  \n",
              " would      2.589365  1.089362 -1.665679  0.050581  \n",
              " president  2.610376  2.396198 -1.990368  3.758069  ,\n",
              " (32917, 10))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = X_train[\"text\"]\n",
        "tokenized_tweet = data.apply(preprocess_text)\n",
        "\n",
        "size=10\n",
        "model_w2v = Word2Vec(\n",
        "            tokenized_tweet, #l'ensemble des tweet\n",
        "            window=5, # context window size\n",
        "            vector_size=size,  #La taille de sortie\n",
        "            min_count=2, # Ignores all words with total frequency lower than 2.\n",
        "            sg = 0) #on utilise la methode CBOW\n",
        "\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(data), epochs=20)\n",
        "\n",
        "#etape5 je stocke mes resultats en dataframe\n",
        "words=list(model_w2v.wv.key_to_index)\n",
        "data_emb2=pd.DataFrame(model_w2v.wv[list(model_w2v.wv.key_to_index)],index=words)\n",
        "\n",
        "data_emb2.head() , data_emb2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIwHHMGdX4oB",
        "outputId": "b16b8407-d4c9-43a8-bd28-fd2828e6a512"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32999"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_emb1.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YkuKv3-Ty17",
        "outputId": "29ae8c92-fe18-4277-8270-d02b7d22c25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('donald', 0.9820414781570435), ('quick', 0.8882084488868713), ('scarcely', 0.8743790984153748), ('presidentelect', 0.8664038777351379), ('reset', 0.8651153445243835), ('bashing', 0.8577965497970581), ('miss', 0.8514626622200012), ('realdonaldtrump', 0.850972056388855), ('insisted', 0.849139928817749), ('impeached', 0.8454713225364685)]\n"
          ]
        }
      ],
      "source": [
        "print(model_w2v.wv.most_similar(\"trump\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzEgjjLrT2Ak"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SteqC55MUad1"
      },
      "source": [
        "## use pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhNeDexHUdxO"
      },
      "outputs": [],
      "source": [
        "# Charger les vecteurs GloVe\n",
        "'''import gensim.downloader\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "# Créer un modèle Word2Vec avec les vecteurs GloVe comme poids initiaux\n",
        "model_w2v = Word2Vec(vector_size=25, min_count=1)\n",
        "model_w2v.build_vocab_from_freq(glove_vectors.vocab)\n",
        "\n",
        "# Mettre à jour le modèle Word2Vec avec vos données\n",
        "model_w2v.build_vocab(tokenized_tweet, update=True)\n",
        "model_w2v.train(tokenized_tweet, total_examples=model_w2v.corpus_count, epochs=20) '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPcI3YivUwBK"
      },
      "outputs": [],
      "source": [
        "#print(model_w2v.wv.most_similar(\"trump\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "WHwTjeo2CyKv"
      },
      "outputs": [],
      "source": [
        "# calculer la moyenn de chaque mot, si le size = 10 donc on\n",
        "def word_doc(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "            vec+= model_w2v.wv[word].reshape((1, size))\n",
        "            count += 1.\n",
        "\n",
        "    if count != 0:\n",
        "        vec = vec/count\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPAOP3hgXdLt",
        "outputId": "5798b519-5228-4a73-a88c-3815defb1889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       [giddy, news, egyptian, commercial, airplane, ...\n",
              "1       [washington, reuters, republicans, us, congres...\n",
              "2       [could, easily, donald, trump, get, elected, n...\n",
              "3              [wow, epic, truthful, bravo, bill, reilly]\n",
              "4       [weekly, documentary, film, curated, editorial...\n",
              "                              ...                        \n",
              "5289    [jr, smith, century, wiremuch, made, democrat,...\n",
              "5290    [bratislava, reuters, farright, people, partyo...\n",
              "5291    [lead, solution, kind, european, union, dictat...\n",
              "5292    [vancouver, reuters, canada, sending, hundreds...\n",
              "5293    [reuters, north, carolina, republican, lawmake...\n",
              "Name: text, Length: 5278, dtype: object"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "vDqZAW1aCyKw",
        "outputId": "d1c2ec36-0696-4fae-e805-a98dd5783ea3"
      },
      "outputs": [],
      "source": [
        "wordvec_arrays = np.zeros((len(tokenized_tweet), size))\n",
        "\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    wordvec_arrays[i,:] = word_doc(tokenized_tweet[i], size)\n",
        "\n",
        "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xYj7DREuCyKq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
